{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from path import Path\n",
    "import gc\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation, LeakyReLU\n",
    "\n",
    "get_custom_objects().update({'leaky-relu':Activation(LeakyReLU(alpha  = 0.2))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_cleanup(objects):\n",
    "    if objects:\n",
    "        del(objects)\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    input_path = Path('../')\n",
    "    dae_batch_size = 128\n",
    "    dae_num_epoch = 50\n",
    "    dae_architecture = [1500, 1500,1500]\n",
    "    reuse_autoencoder = False\n",
    "    batch_size = 128\n",
    "    num_epocs = 150\n",
    "    units = [64, 32]\n",
    "    input_dropout = 0.06\n",
    "    dropout = 0.08\n",
    "    regL2 = 0.09\n",
    "    activation = 'relu'\n",
    "\n",
    "    cv_folds = 5\n",
    "    nas = True\n",
    "    random_state = 0\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.input_path/'input/train.csv', index_col= 'id')\n",
    "test = pd.read_csv(config.input_path/'input/test.csv', index_col= 'id')\n",
    "submission = pd.read_csv(config.input_path/'input/sample_submission.csv', index_col= 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_features = [feat for feat in train.columns if \"_calc\" in feat]\n",
    "cat_features = [feat for feat in train.columns if \"_cat\" in feat]\n",
    "target = train[\"target\"]\n",
    "train = train.drop(\"target\", axis=\"columns\")\n",
    "train = train.drop(calc_features, axis=\"columns\")\n",
    "test = test.drop(calc_features, axis=\"columns\")\n",
    "train = pd.get_dummies(train, columns=cat_features)\n",
    "test = pd.get_dummies(test, columns=cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying gaussrank to columns: ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15']\n"
     ]
    }
   ],
   "source": [
    "print(\"applying gaussrank to columns: \", end = '')\n",
    "to_normalize = list()\n",
    "\n",
    "for k, col in enumerate(train.columns):\n",
    "    if '_bin' not in col and '_cat' not in col and '_missing' not in col:\n",
    "        to_normalize.append(col)\n",
    "print(to_normalize)\n",
    "\n",
    "def to_gauss(x): return np.sqrt(2) * erfinv(x)\n",
    "def normalize(data, norm_cols):\n",
    "    n = data.shape[0]\n",
    "    for col in norm_cols:\n",
    "        sorted_idx = data[col].sort_values().index.tolist()\n",
    "        uniform = np.linspace(start = -0.99, stop = 0.99, num = n)\n",
    "        normal = to_gauss(uniform)\n",
    "        normalized_col = pd.Series(index = sorted_idx, data = normal)\n",
    "        data[col] = normalized_col\n",
    "    return data\n",
    "\n",
    "train = normalize(train, to_normalize)\n",
    "test = normalize(test, to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns\n",
    "train_index = train.index\n",
    "test_index = test.index\n",
    "train = train.values.astype(np.float32)\n",
    "test = test.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuksy\\AppData\\Local\\Temp\\ipykernel_21760\\2068861814.py:31: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n"
     ]
    }
   ],
   "source": [
    "def plot_keras_history(history, measures):\n",
    "    rows = len(measures)//2 + len(measures) % 2\n",
    "    fig, panels = plt.subplots(rows, 2, figsize = (15,5))\n",
    "    plt.subplots_adjust(top = 0.99, bottom = 0.01, hspace = 0.4, wspace = 0.2)\n",
    "\n",
    "    try:\n",
    "        panels = [item for sublist in panels for item in sublist]\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for k, measure in enumerate(measures):\n",
    "        panel = panels[k]\n",
    "        panel.set_title(measure + 'history')\n",
    "        panel.plot(history.epoch, history.history['measure'], label = 'train '+measure)\n",
    "\n",
    "        try:\n",
    "            panel.plot(history.epoch,\n",
    "                       history.history['val_'+measure],\n",
    "                       label = 'validation '+measure)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        panel.set(xlabel = 'epochs', ylabel = measure)\n",
    "        panel.legend()\n",
    "\n",
    "    plt.show(fig)\n",
    "\n",
    "\n",
    "from numba import jit\n",
    "@jit\n",
    "def eval_gini(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_pred)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x, batch_size, shuffle = True, random_state = None):\n",
    "    \n",
    "    batch_index = 0 #initial batch starts at reference 0\n",
    "    n = x.shape[0] #total length of the data that is going generate the batchs\n",
    "\n",
    "    while True: #keep going\n",
    "        if batch_index == 0: #first run\n",
    "            index_array = np.arange(n) #setting the ordered index of the data \n",
    "            \n",
    "            if shuffle: #in case of shuffle, set the seed and shuffle the index\n",
    "                np.random.seed(seed = random_state)\n",
    "                index_array = np.random.permutation(n) #permutation has something similar to np.arange when used in integers\n",
    "\n",
    "            current_index = (batch_index * batch_size) % n #the current index is going to drop only when surprass n\n",
    "            if n>= current_index + batch_size: # if its not, then keep updating the batch_index\n",
    "                current_batch_size = batch_size\n",
    "                batch_index += 1\n",
    "\n",
    "            else: #when the actual batch surpass the number of data\n",
    "                current_batch_size = n - current_index #how many points are overpassing n\n",
    "                batch_index = 0 #return to the first batch\n",
    "            \n",
    "            batch = x[index_array[current_index:current_index+current_batch_size]] #\n",
    "\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_generator(X, batch_size, swaprate=0.15, shuffle=True, random_state=None):\n",
    "    if random_state is None:\n",
    "        random_state = np.randint(0, 999)\n",
    "    num_features = X.shape[1]\n",
    "    num_swaps = int(num_features * swaprate)    \n",
    "    generator_a = batch_generator(X, batch_size, shuffle, \n",
    "                                  random_state)\n",
    "    generator_b = batch_generator(X, batch_size, shuffle, \n",
    "                                  random_state + 1)\n",
    "    while True:\n",
    "        batch = next(generator_a)\n",
    "        mixed_batch = batch.copy()\n",
    "        effective_batch_size = batch.shape[0]\n",
    "        alternative_batch = next(generator_b)\n",
    "        assert((batch != alternative_batch).any())\n",
    "        for i in range(effective_batch_size):\n",
    "            swap_idx = np.random.choice(num_features, num_swaps, \n",
    "                                        replace=False)\n",
    "            mixed_batch[i, swap_idx] = alternative_batch[i, swap_idx]\n",
    "        yield (mixed_batch, batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DAE(X, architecture = [1500,1500,1500]):\n",
    "    features = X.shape[1]\n",
    "    inputs = Input((features,))\n",
    "\n",
    "    for i, nodes in enumerate(architecture):\n",
    "        layer = Dense(nodes,\n",
    "                       activation = 'relu',\n",
    "                       use_bias = False,\n",
    "                       name = f'code_{i+1}')\n",
    "        \n",
    "        if i == 0:\n",
    "            x = layer(inputs)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "    outputs = Dense(features,activation = 'linear')(x)\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dae_features(autoencoder, X, layers = [3], batch_size = 128):\n",
    "    data = []\n",
    "    for layer in layers:\n",
    "        if layer == 0:\n",
    "            data.append(X)\n",
    "        else:\n",
    "            get_layer_output = Model([autoencoder.layers[0].input],\n",
    "                                     [autoencoder.layers[layer].output])\n",
    "            \n",
    "            layer_output = get_layer_output.predict(X, batch_size = batch_size)\n",
    "\n",
    "            data.append(layer_output)\n",
    "        data = np.hstack(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_fit(X_train, X_valid, filename = 'dae',random_state = None, suppress_output = False):\n",
    "    if suppress_output:\n",
    "        verbose = 0\n",
    "    else:\n",
    "        verbose = 2\n",
    "        print('fitting a denoise autoencoder')\n",
    "        \n",
    "    tf.random.set_seed(seed = random_state)\n",
    "\n",
    "    generator = mixup_generator(X_train,\n",
    "                                batch_size=config.batch_size,\n",
    "                                swaprate=0.15,\n",
    "                                random_state=config.random_state)\n",
    "    \n",
    "    dae = get_DAE(X_train, architecture=config.dae_architecture)\n",
    "\n",
    "    steps_per_epoch = np.ceil(X_train.shape[0]/config.dae_batch_size)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = 'val_mse',\n",
    "                                   mode = 'min',\n",
    "                                   patience = 5,\n",
    "                                   restore_best_weights = True,\n",
    "                                   verbose = 0)\n",
    "    \n",
    "\n",
    "    history = dae.fit(generator,\n",
    "                      steps_per_epoch = steps_per_epoch,\n",
    "                      epochs = config.num_epocs, \n",
    "                      validation_data = (X_valid, X_valid),\n",
    "                      verbose = verbose)\n",
    "\n",
    "    if not suppress_output:\n",
    "        plot_keras_history(history, measures = ['mse','mae'])\n",
    "\n",
    "    dae.save(filename)\n",
    "    return dae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_blocks(x, units, activation, regL2, dropout):\n",
    "    kernel_initializer = keras.initializers.RandomNormal(mean = 0.0,\n",
    "                                                         stddev = 0.01, seed = config.random_state)\n",
    "    \n",
    "    for k,layer_units in enumerate(units):\n",
    "        if regL2 >0:\n",
    "            x = Dense(layer_units, activation=activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                      kernel_regularizer = l2(regL2))(x)\n",
    "            \n",
    "        else:\n",
    "            x = Dense(layers_units, activation = activation,\n",
    "                      kernel_initializer = kernel_initializer)(x)\n",
    "            \n",
    "        if dropout > 0:\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(dae, units = [4500,1000,1000],\n",
    "              input_dropout = 0.1, dropout = 0.5,\n",
    "              regL2 = 0.05,\n",
    "              activation = 'relu'):\n",
    "    \n",
    "    inputs = dae.get_layer('code_2').output\n",
    "    \n",
    "    if input_dropout > 0 :\n",
    "        x = Dropout(input_dropout)(inputs)\n",
    "\n",
    "    else:\n",
    "        x = tf.keras.layers.Layer()(inputs)\n",
    "        \n",
    "    x = dense_blocks(x, units, activation, regL2, dropout)\n",
    "    outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs = dae.input, outputs = outputs)    \n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss = keras.losses.binary_crossentropy,\n",
    "                  metrics = [AUC(name = 'auc')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(X_train, y_train, X_valid, y_valid,\n",
    "                  autoencoder,\n",
    "                  filename,\n",
    "                  random_state = None,\n",
    "                  suppress_output = False):\n",
    "    \n",
    "    if suppress_output:\n",
    "        verbose = 0\n",
    "\n",
    "    else:\n",
    "        print('fitting model')\n",
    "        verbose = 2\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_auc',\n",
    "                                   mode = 'max',\n",
    "                                   patience = 10,\n",
    "                                   restore_best_weights = True,\n",
    "                                   verbose = 0)\n",
    "    \n",
    "    rlrop = ReduceLROnPlateau(monitor = 'val_auc',\n",
    "                              mode = 'max',\n",
    "                              patience = 2,\n",
    "                              factor = 0.75,\n",
    "                              verbose = 0)\n",
    "    \n",
    "    tf.random.set_seed(seed = random_state)\n",
    "    model = dnn_model(autoencoder,\n",
    "                      units = config.units,\n",
    "                      input_dropout=config.input_dropout,\n",
    "                      dropout=config.dropout,\n",
    "                      regL2 = config.regL2,\n",
    "                      activation=config.activation)\n",
    "    \n",
    "    history = model.fit(X_train,y_train,\n",
    "                        epochs = config.num_epocs,\n",
    "                        batch_size = config.batch_size,\n",
    "                        validation_data = (X_valid, y_valid),\n",
    "                        callbacks = [early_stopping, rlrop],\n",
    "                        shuffle = True,\n",
    "                        verbose = verbose)\n",
    "    model.save(filename)\n",
    "\n",
    "    if not suppress_output:\n",
    "        plot_keras_history(history, measures = ['loss','auc'])\n",
    "\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.nas is True:\n",
    "    def evaluate():\n",
    "        metric_evaluations = list()\n",
    "        skf = StratifiedKFold(n_splits = config.cv_folds,\n",
    "                              shuffle=True,\n",
    "                              random_state=config.random_state)\n",
    "        #print(train.shape, target.shape)\n",
    "        for k, (train_idx, valid_idx) in enumerate(skf.split(train,target)):\n",
    "            X_train, y_train = train[train_idx,:], target.iloc[train_idx]\n",
    "            X_valid, y_valid = train[valid_idx,:], target.iloc[valid_idx]\n",
    "\n",
    "            if config.reuse_autoencoder:\n",
    "                autoencoder = load_model(f\"./dae_fold{k}\")\n",
    "            else:\n",
    "                autoencoder = autoencoder_fit(X_train,X_valid,\n",
    "                                              filename= f'./dae_fold{k}',\n",
    "                                              random_state=config.random_state,\n",
    "                                              suppress_output=True)\n",
    "                \n",
    "            model,_ = model_fitting(X_train,y_train, X_valid,y_valid,\n",
    "                                    autoencoder=autoencoder,\n",
    "                                    filename = f'dnn_model_fold{k}',\n",
    "                                    random_state = config.random_state,\n",
    "                                    suppress_output = True)\n",
    "            \n",
    "            val_preds = model.predict(X_valid, batch_size = 128, verbose = 0)\n",
    "            best_score = eval_gini(y_true = y_valid, y_pred = np.ravel(val_preds))\n",
    "            metric_evaluations.append(best_score)\n",
    "            gpu_cleanup([autoencoder, model])\n",
    "\n",
    "        return np.mean(metric_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-20 09:00:44,779] A new study created in memory with name: no-name-d0fb1244-683a-43e2-b726-e6d52e00f1a0\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {'first_layer':trial.suggest_categorical('first_layer',[8,16,32,64,128,256,512]),\n",
    "              'second_layer':trial.suggest_categorical('second_layer',[0,8,16,32,64,128,256]),\n",
    "              'third_layer':trial.suggest_categorical('third_layer',[0,8,16,32,64,128,256]),\n",
    "              'input_drop':trial.suggest_float('input_droput',0.0, 0.5),\n",
    "              'dropout':trial.suggest_float('dropout', 0.0, 0.5),\n",
    "              'regL2':trial.suggest_float('regL2',0.0, 0.1),\n",
    "              'activation':trial.suggest_categorical('activation',['relu','leaky-relu','selu'])}\n",
    "    \n",
    "    config.units = [nodes for nodes in [params['first_layer'],\n",
    "                                        params['second_layer'],\n",
    "                                        params['third_layer']] if nodes >0 ]\n",
    "    \n",
    "    config.input_dropout = params['input_drop']\n",
    "    config.dropout = params['dropout']\n",
    "    config.regL2 = params['regL2']\n",
    "    config.activation = params['activation']\n",
    "\n",
    "    return evaluate()\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective, n_trials=60)\n",
    "print('best gini score', study.best_value)\n",
    "print('best params', study.best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.units = [nodes for nodes in [study.best_params['first_layer'],\n",
    "                                     study.best_params['second_layer'],\n",
    "                                       study.best_params['third_layer']] if nodes > 0]\n",
    "\n",
    "config.input_dropout = study.best_params['input_dropout']\n",
    "    config.dropout = study.best_params['dropout']\n",
    "    config.regL2 = study.best_params['regL2']\n",
    "    config.activation = study.best_params['activation']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
